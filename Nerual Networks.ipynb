{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural NetWorks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data for Neural Networks  \n",
    "Notes: Data Preprocessing is deadly important for neural networks! Please remember it!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                     [-200.2, -234.1],\n",
    "                     [5000.5, 150.1],\n",
    "                     [6000.6, -125.1],\n",
    "                     [9000.9, -673.1]])\n",
    "# Create scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Transform the feature\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "# Show feature\n",
    "features_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\",\n",
    "input_shape=(10,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) =\n",
    "imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "# Convert movie review data to one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,\n",
    "                                               mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,\n",
    "                                              mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\",\n",
    "                         input_shape=(\n",
    "                             number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\",  # Cross-entropy\n",
    "                optimizer=\"rmsprop\",  # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"])  # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train,  # Features\n",
    "                      target_train,  # Target vector\n",
    "                      epochs=3,  # Number of epochs\n",
    "                      verbose=1,  # Print description after each epoch\n",
    "                      batch_size=100,  # Number of observations per batch\n",
    "                      validation_data=(features_test,\n",
    "                                       target_test))  # Test data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Multiclass Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 5000\n",
    "# Load feature and target data\n",
    "data = reuters.load_data(num_words=number_of_features)\n",
    "(data_train, target_vector_train), (data_test,\n",
    "                                    target_vector_test) = data\n",
    "# Convert feature data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,\n",
    "                                               mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,\n",
    "                                              mode=\"binary\")\n",
    "# One-hot encode target vector to create a target matrix\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\",  # Crossentropy\n",
    "                optimizer=\"rmsprop\",  # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"])  # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train,  # Features\n",
    "                      target_train,  # Target\n",
    "                      epochs=3,  # Three epochs\n",
    "                      verbose=0,  # No output\n",
    "                      batch_size=100,  # Number of observations per batch\n",
    "                      validation_data=(features_test,\n",
    "                                       target_test))  # Test data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_regression(n_samples=10000,\n",
    "                                   n_features=3,\n",
    "                                   n_informative=3,\n",
    "                                   n_targets=1,\n",
    "                                   noise=0.0,\n",
    "                                   random_state=0)\n",
    "# Divide our data into training and test sets\n",
    "features_train, features_test, target_train, target_test =\n",
    "train_test_split(\n",
    "    features, target, test_size=0.33, random_state=0)\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=32,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(features_train.shape[1],)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=32, activation=\"relu\"))\n",
    "# Add fully connected layer with no activation function\n",
    "network.add(layers.Dense(units=1))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"mse\",  # Mean squared error\n",
    "                optimizer=\"RMSprop\",  # Optimization algorithm\n",
    "                metrics=[\"mse\"])  # Mean squared error\n",
    "# Train neural network\n",
    "history = network.fit(features_train,  # Features\n",
    "                      target_train,  # Target vector\n",
    "                      epochs=10,  # Number of epochs\n",
    "                      verbose=0,  # No output\n",
    "                      batch_size=100,  # Number of observations per batch\n",
    "                      validation_data=(features_test,\n",
    "                                       target_test))  # Test data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 10000\n",
    "# Load data and target vector from IMDB movie data\n",
    "(data_train, target_train), (data_test, target_test) =\n",
    "imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "# Convert IMDB data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,\n",
    "                                               mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,\n",
    "                                              mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\",  # Cross-entropy\n",
    "                optimizer=\"rmsprop\",  # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"])  # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train,  # Features\n",
    "                      target_train,  # Target vector\n",
    "                      epochs=3,  # Number of epochs\n",
    "                      verbose=0,  # No output\n",
    "                      batch_size=100,  # Number of observations\n",
    "                      per batch\n",
    "                      validation_data=(features_test,\n",
    "                                       target_test))  # Test data\n",
    "# Predict classes of test set\n",
    "predicted_target = network.predict(features_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 10000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) =\n",
    "imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "# Convert movie review data to a one-hot encoded feature\n",
    "matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,\n",
    "                                               mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,\n",
    "                                              mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\",  # Cross-entropy\n",
    "                optimizer=\"rmsprop\",  # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"])  # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train,  # Features\n",
    "                      target_train,  # Target\n",
    "                      epochs=15,  # Number of epochs\n",
    "                      verbose=0,  # No output\n",
    "                      batch_size=1000,  # Number of\n",
    "                      observations per batch\n",
    "                      validation_data=(features_test,\n",
    "                                       target_test))  # Test data\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, \"r--\")\n",
    "plt.plot(epoch_count, test_loss, \"b-\")\n",
    "plt.legend([\"Training Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "```\n",
    "Alternatively, we can use the same approach to visualize the training and test\n",
    "accuracy over each epoch:\n",
    "```python\n",
    "# Get training and test accuracy histories\n",
    "training_accuracy = history.history[\"acc\"]\n",
    "test_accuracy = history.history[\"val_acc\"]\n",
    "plt.plot(epoch_count, training_accuracy, \"r--\")\n",
    "plt.plot(epoch_count, test_accuracy, \"b-\")\n",
    "# Visualize accuracy history\n",
    "plt.legend([\"Training Accuracy\", \"Test Accuracy\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.show();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Overfitting with Weight Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) =\n",
    "imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,\n",
    "                                               mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,\n",
    "                                              mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLu activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         kernel_regularizer=regularizers.l2(0.01),\n",
    "                         input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, kernel_regularizer=regularizers.l2(0.01),\n",
    "                         activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\",  # Cross-entropy\n",
    "                optimizer=\"rmsprop\",  # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"])  # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train,  # Features\n",
    "                      target_train,  # Target vector\n",
    "                      epochs=3,  # Number of epochs\n",
    "                      verbose=0,  # No output\n",
    "                      batch_size=100,  # Number of observations per batch\n",
    "                      validation_data=(features_test,\n",
    "                                       target_test))  # Test data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reducing Overfitting with Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) =\n",
    "imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,\n",
    "                                               mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\",  # Cross-entropy\n",
    "                optimizer=\"rmsprop\",  # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"])  # Accuracy performance metric\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "             ModelCheckpoint(filepath=\"best_model.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True)]\n",
    "# Train neural network\n",
    "history = network.fit(features_train,  # Features\n",
    "                      target_train,  # Target vector\n",
    "                      epochs=20,  # Number of epochs\n",
    "                      callbacks=callbacks,  # Early stopping\n",
    "                      verbose=0,  # Print description after each epoch\n",
    "                      batch_size=100,  # Number of observations per batch\n",
    "                      validation_data=(features_test,\n",
    "                                       target_test))  # Test data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Overfitting with Dropout  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) =\n",
    "imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,\n",
    "                                               mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,\n",
    "                                              mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add a dropout layer for input layer\n",
    "network.add(layers.Dropout(0.2, input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(layers.Dropout(0.5))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(layers.Dropout(0.5))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\",  # Cross-entropy\n",
    "                optimizer=\"rmsprop\",  # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"])  # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train,  # Features\n",
    "                      target_train,  # Target vector\n",
    "                      epochs=3,  # Number of epochs\n",
    "                      verbose=0,  # No output\n",
    "                      batch_size=100,  # Number of observations per batch\n",
    "                      validation_data=(features_test,\n",
    "                                       target_test))  # Test data\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model Training Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) =\n",
    "imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "# Convert movie review data to a one-hot encoded feature\n",
    "matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,\n",
    "mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,\n",
    "mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "activation=\"relu\",\n",
    "input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square\n",
    "Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance\n",
    "metric\n",
    "# Set callback functions to early stop training and save the\n",
    "best model so far\n",
    "checkpoint = [ModelCheckpoint(filepath=\"models.hdf5\")]\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target vector\n",
    "epochs=3, # Number of epochs\n",
    "callbacks=checkpoint, # Checkpoint\n",
    "verbose=0, # No output\n",
    "batch_size=100, # Number of observations\n",
    "per batch\n",
    "validation_data=(features_test,\n",
    "target_test)) # Test data\n",
    "```\n",
    "\n",
    "## k-Fold Cross-Validating Neural Networks\n",
    "```python\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Number of features\n",
    "number_of_features = 100\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "n_features =\n",
    "number_of_features,\n",
    "n_informative = 3,\n",
    "n_redundant = 0,\n",
    "n_classes = 2,\n",
    "weights = [.5, .5],\n",
    "random_state = 0)\n",
    "# Create function returning a compiled network\n",
    "def create_network():\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation\n",
    "function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\",\n",
    "input_shape=(\n",
    "number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation\n",
    "function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation\n",
    "function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Crossentropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square\n",
    "Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy\n",
    "performance metric\n",
    "# Return compiled network\n",
    "return network\n",
    "# Wrap Keras model so it can be used by scikit-learn\n",
    "neural_network = KerasClassifier(build_fn=create_network,\n",
    "epochs=10,\n",
    "batch_size=100,\n",
    "verbose=0)\n",
    "# Evaluate neural network using three-fold cross-validation\n",
    "cross_val_score(neural_network, features, target, cv=3)\n",
    "```\n",
    "\n",
    "## Tuning Neural Networks\n",
    "``` python \n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Number of features\n",
    "number_of_features = 100\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "n_features =\n",
    "number_of_features,\n",
    "n_informative = 3,\n",
    "n_redundant = 0,\n",
    "n_classes = 2,\n",
    "weights = [.5, .5],\n",
    "random_state = 0)\n",
    "# Create function returning a compiled network\n",
    "def create_network(optimizer=\"rmsprop\"):\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation\n",
    "function\n",
    "network.add(layers.Dense(units=16,\n",
    "activation=\"relu\",\n",
    "input_shape=\n",
    "(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation\n",
    "function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation\n",
    "function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Crossentropy\n",
    "optimizer=optimizer, # Optimizer\n",
    "metrics=[\"accuracy\"]) # Accuracy\n",
    "performance metric\n",
    "# Return compiled network\n",
    "return network\n",
    "# Wrap Keras model so it can be used by scikit-learn\n",
    "neural_network = KerasClassifier(build_fn=create_network,\n",
    "verbose=0)\n",
    "# Create hyperparameter space\n",
    "epochs = [5, 10]\n",
    "batches = [5, 10, 100]\n",
    "optimizers = [\"rmsprop\", \"adam\"]\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(optimizer=optimizers, epochs=epochs,\n",
    "batch_size=batches)\n",
    "grid = GridSearchCV(estimator=neural_network,\n",
    "param_grid=hyperparameters)\n",
    "# Fit grid search\n",
    "grid_result = grid.fit(features, target)\n",
    "```\n",
    "\n",
    "## Visualizing Neural Networks\n",
    "```python\n",
    "# Load libraries\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\",\n",
    "input_shape=(10,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Visualize network architecture\n",
    "SVG(model_to_dot(network, show_shapes=True).create(prog=\"dot\",\n",
    "format=\"svg\"))\n",
    "# Save the visualization as a file\n",
    "plot_model(network, show_shapes=True, to_file=\"network.png\")\n",
    "# Visualize network architecture\n",
    "SVG(model_to_dot(network,\n",
    "show_shapes=False).create(prog=\"dot\", format=\"svg\"))\n",
    "```\n",
    "\n",
    "## Classifying images\n",
    "```python\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "# Set that the color channel value will be first\n",
    "K.set_image_data_format(\"channels_first\")\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "# Set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "# Load data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) =\n",
    "mnist.load_data()\n",
    "# Reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], channels,\n",
    "height, width)\n",
    "# Reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], channels,\n",
    "height, width)\n",
    "# Rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "# One-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "# Start neural network\n",
    "network = Sequential()\n",
    "# Add convolutional layer with 64 filters, a 5x5 window, and\n",
    "ReLU activation function\n",
    "network.add(Conv2D(filters=64,\n",
    "kernel_size=(5, 5),\n",
    "input_shape=(channels, width, height),\n",
    "activation='relu'))\n",
    "# Add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "# Add layer to flatten input\n",
    "network.add(Flatten())\n",
    "# # Add fully connected layer of 128 units with a ReLU\n",
    "activation function\n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Crossentropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Train neural network\n",
    "network.fit(features_train, # Features\n",
    "target_train, # Target\n",
    "epochs=2, # Number of epochs\n",
    "verbose=0, # Don't print description after each epoch\n",
    "batch_size=1000, # Number of observations per batch\n",
    "validation_data=(features_test, target_test))\n",
    "```\n",
    "\n",
    "## Improving Performance with Image Augmentation\n",
    "``` python \n",
    "# Load library\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Create image augmentation\n",
    "augmentation = ImageDataGenerator(featurewise_center=True, #\n",
    "Apply ZCA whitening\n",
    "zoom_range=0.3, # Randomly\n",
    "zoom in on images\n",
    "width_shift_range=0.2, # Randomly shift images\n",
    "horizontal_flip=True, #Randomly flip images\n",
    "rotation_range=90) # Randomly rotate\n",
    "# Process all images from the directory 'raw/images'\n",
    "augment_images = augmentation.flow_from_directory(\"raw/images\", # Image folder\n",
    "batch_size=32, # Batch size\n",
    "class_mode=\"binary\", # Classes\n",
    "save_to_dir=\"processed/images\")\n",
    "```\n",
    "\n",
    "## Classifying Text\n",
    "```python\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) =\n",
    "imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "# Use padding or truncation to make each observation have 400\n",
    "features\n",
    "features_train = sequence.pad_sequences(data_train,\n",
    "maxlen=400)\n",
    "features_test = sequence.pad_sequences(data_test, maxlen=400)\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add an embedding layer\n",
    "network.add(layers.Embedding(input_dim=number_of_features,\n",
    "output_dim=128))\n",
    "# Add a long short-term memory layer with 128 units\n",
    "network.add(layers.LSTM(units=128))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"Adam\", # Adam optimization\n",
    "metrics=[\"accuracy\"]) # Accuracy performance\n",
    "metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target\n",
    "epochs=3, # Number of epochs\n",
    "verbose=0, # Do not print description after each epoch\n",
    "batch_size=1000, # Number of observations per batch\n",
    "validation_data=(features_test,\n",
    "target_test)) # Test data\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
